import os
from PIL import Image
import numpy as np
import faiss
import torch
from transformers import CLIPProcessor, CLIPModel

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
IMAGE_DIR = "images/"
INDEX_FILE = "index.faiss"
PATHS_FILE = "image_paths.npy"

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
print("ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ CLIP...")
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØµØ§ÙˆÛŒØ±
embeddings = []
image_paths = []

print("ğŸ–¼ï¸ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµØ§ÙˆÛŒØ±...")
for filename in os.listdir(IMAGE_DIR):
    filepath = os.path.join(IMAGE_DIR, filename)
    if filename.lower().endswith((".png", ".jpg", ".jpeg", ".webp")):
        try:
            image = Image.open(filepath).convert("RGB")
            inputs = processor(images=image, return_tensors="pt")
            with torch.no_grad():
                outputs = model.get_image_features(**inputs)
            vector = outputs[0].numpy()
            embeddings.append(vector)
            image_paths.append(filepath)
            print(f"âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯: {filename}")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± {filename}: {e}")

# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡
embedding_matrix = np.array(embeddings).astype("float32")

# Ø³Ø§Ø®Øª Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS
print("ğŸ“¦ Ø³Ø§Ø®Øª FAISS index...")
index = faiss.IndexFlatL2(embedding_matrix.shape[1])  # ÙØ§ØµÙ„Ù‡ Ø§Ù‚Ù„ÛŒØ¯Ø³ÛŒ
index.add(embedding_matrix)
faiss.write_index(index, INDEX_FILE)

# Ø°Ø®ÛŒØ±Ù‡ Ù…Ø³ÛŒØ± Ø¹Ú©Ø³â€ŒÙ‡Ø§
np.save(PATHS_FILE, np.array(image_paths))

print("ğŸ‰ Ù‡Ù…Ù‡â€ŒÚ†ÛŒØ² Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Øª! Ø­Ø§Ù„Ø§ Ø³Ø±ÙˆØ± Ø±Ùˆ Ø§Ø¬Ø±Ø§ Ú©Ù† Ùˆ Ø¨Ø§ Postman ØªØ³Øª Ú©Ù†.")
